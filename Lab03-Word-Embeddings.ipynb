{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reflected-workshop",
   "metadata": {},
   "source": [
    "# Lab 03 Part 2 - Word Embeddings\n",
    "In this lab we will look into word embeddings with word2vec and other similar methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d98242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2025/blob/main/lab03/lab03_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "colab_button = HTML(\n",
    "    '<a href=\"https://colab.research.google.com/github/surrey-nlp/NLP-2025/blob/main/lab03/lab03_word_embeddings.ipynb\" target=\"_parent\">'\n",
    "    '<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>'\n",
    ")\n",
    "display(colab_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-jumping",
   "metadata": {},
   "source": [
    "### Build your own\n",
    "Let's start by first building out own word2vec model, instead of downloading a ready trained one. For that we are going to use the 20 news groups from sklearn, since is not too large for a lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "# lets check the first two documents\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-basement",
   "metadata": {},
   "source": [
    "The first thing to do is to format the documents into a list of sentences that contains a list of tokens. We are not going to do any further cleaning and pre-processing for now (to keep things simple for the labs), but that would be advisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# This will take a minute or so\n",
    "token_list = []\n",
    "## TODO: use this loop to tokenize each document using sent_tokenize, and then to tokenize each word using word_tokenize; \n",
    "##       do remember to add all the resultant tokens to token_list\n",
    "\n",
    "for d in documents:\n",
    "    s = ...\n",
    "    token_list = ...\n",
    "\n",
    "# check the first three sentences\n",
    "token_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-spain",
   "metadata": {},
   "source": [
    "Now is time to import the word2vec algorithm and set the key parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Number of vector elements (dimensions) to represent the word vector\n",
    "num_features = 300\n",
    "# Min number of word count to be considered in the Word2vec model. If your corpus is small, reduce the min count. If you’re training with a large corpus, increase the min count.\n",
    "min_word_count = 1\n",
    "# Number of CPU cores used for the training. If you want to set the number of cores dynamically, check out import multiprocessing: \n",
    "#num_workers = multiprocessing.cpu_count()\n",
    "num_workers = 2\n",
    "# Context window size\n",
    "window_size = 3\n",
    "# Subsampling rate for frequent terms\n",
    "subsampling = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-calcium",
   "metadata": {},
   "source": [
    "Let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## TODO: Use the parameters defined in the code cell above above to start Word2Vec model training.\n",
    "model = Word2Vec(token_list, workers=..., vector_size=..., min_count=..., window=..., sample=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-technical",
   "metadata": {},
   "source": [
    "Once you’ve trained your word model, you can reduce the memory footprint by about half if you freeze your model and discard the unnecessary information. The following command will discard the unneeded output weights of your neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-freeware",
   "metadata": {},
   "source": [
    "The model cannot be trained further once the weights of the output layer have been discarded.\n",
    "\n",
    "Save the trained model with the following command and preserve it for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"my_own_domain_specific_word2vec_model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-necklace",
   "metadata": {},
   "source": [
    "Now lets say we want to load the model that we had previously saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model_name = \"my_own_domain_specific_word2vec_model\"\n",
    "## TODO: Load the model you just saved in your lab folder\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-pepper",
   "metadata": {},
   "source": [
    "Let's check the most similar words to \"justice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('justice'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-parade",
   "metadata": {},
   "source": [
    "### Challenge - 1\n",
    "Try a few more words and observe if what is retrieved makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-boundary",
   "metadata": {},
   "source": [
    "### Using the gensim API\n",
    "Having build our own model is great, but lets now load a model that was trained with MANY documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-beauty",
   "metadata": {},
   "source": [
    "We will be using the downloader for the embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# this command can be used to check what models are available\n",
    "#api.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-aurora",
   "metadata": {},
   "source": [
    "Let's load the word2vec model from google news containing 300 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will also take a minute or so\n",
    "## TODO: Use the api above to load a new model 'word2vec-google-news-300'\n",
    "word2vec_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-wiring",
   "metadata": {},
   "source": [
    "Now check the embedding vector for \"beautiful\"... you will see a 300 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model[\"beautiful\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-kruger",
   "metadata": {},
   "source": [
    "Let's check some similar words to the word \"girl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-reception",
   "metadata": {},
   "source": [
    "How about some maths with vectors! Try the following:\n",
    "\n",
    "queen - girl + boy = king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar(positive=['boy', 'queen'], negative=['girl'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-nature",
   "metadata": {},
   "source": [
    "Time to do some visualisations and see how similar words end up close together and far from other words that are not as similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\", \"banana\", \"apple\", \"mango\", \"fruit\", \"coconut\", \"orange\"]\n",
    "\n",
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    wordvecs = []\n",
    "\n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42)\n",
    "    coordinates = tsne_model.fit_transform(np.array(wordvecs))\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in coordinates:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(8,8)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(2, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "tsne_plot(word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-school",
   "metadata": {},
   "source": [
    "### Challenge - 2\n",
    "Try a few more examples to visualise and see if similar words land close together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-tribute",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "Let's try another model (GloVe) and see if that is any different to word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model[\"beautiful\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-adult",
   "metadata": {},
   "source": [
    "It will be interesting to see if this will fins similar words to \"girl\" like word2vec did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-worship",
   "metadata": {},
   "source": [
    "Let's also see if it can solve the same analogy too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['boy', 'queen'], negative=['girl'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\", \"banana\", \"apple\", \"mango\", \"fruit\", \"coconut\", \"orange\"]\n",
    "\n",
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    wordvecs = []\n",
    "\n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42)\n",
    "    coordinates = tsne_model.fit_transform(np.array(wordvecs))\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in coordinates:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(8,8)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(2, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "tsne_plot(glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-faith",
   "metadata": {},
   "source": [
    "Let's continue with GloVe and check if plural words play any role in how close is to the original singular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glove_model.distance(\"fruit\", \"fruits\"))\n",
    "print(glove_model.distance(\"girl\", \"girls\"))\n",
    "print(glove_model.distance(\"girl\", \"boy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-oliver",
   "metadata": {},
   "source": [
    "## Challenge - 3\n",
    "Calculate the distance for \"king\" and \"queen\", then for \"woman and \"man\". Is it similar? Check the plot to confirm.\n",
    "\n",
    "Calculate the distance for \"king\" and \"apple\", then for \"queen\" and \"apple\". Is it similar again? Check the plot to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-stick",
   "metadata": {},
   "source": [
    "Now let's try and see if the model can find the capitals of different countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pretty print function\n",
    "def pp(obj):\n",
    "    print(pd.DataFrame(obj))\n",
    "\n",
    "## TODO: Use the most_similar function over glove_model to find out the capitals of the countries in the list below.\n",
    "##       HINT: [worda] is to be passed as a value to the negative parameter. \n",
    "##       Ques: Should [wordb, wordc] be passed as a positive parameter? \n",
    "##             What should be the topn results returned? Think about this and complete the function.\n",
    "def analogy(worda, wordb, wordc):\n",
    "    result = ...\n",
    "    return result[0][0]\n",
    "\n",
    "countries = ['australia', 'canada', 'germany', 'ireland', 'italy']\n",
    "capitals = [analogy('usa', 'washington', country) for country in countries]\n",
    "pp(zip(countries,capitals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-cache",
   "metadata": {},
   "source": [
    "## Challenge - 4\n",
    "Looks good... but what if you change \"usa\" to \"us\"? Or if you used a different example to start with like \"greece\" and \"athens\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-intelligence",
   "metadata": {},
   "source": [
    "Now let's plot the results on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_data(orig_data, labels):\n",
    "    pca = PCA(n_components=2)\n",
    "    data = pca.fit_transform(orig_data)\n",
    "    plt.figure(figsize=(7, 5), dpi=100)\n",
    "    plt.plot(data[:,0], data[:,1], '.')\n",
    "    for i in range(len(data)):\n",
    "        plt.annotate(labels[i], xy = data[i])\n",
    "    for i in range(len(data)//2):\n",
    "        plt.annotate(\"\",\n",
    "                xy=data[i],\n",
    "                xytext=data[i+len(data)//2],\n",
    "                arrowprops=dict(arrowstyle=\"->\",\n",
    "                                connectionstyle=\"arc3\")\n",
    "        )\n",
    "       \n",
    "labels = countries + capitals\n",
    "data = [glove_model[w] for w in labels]\n",
    "plot_data(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-killer",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "Now let's look into generating feature vectors for documents instead of just words. For that we are going to use word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument,Doc2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-external",
   "metadata": {},
   "source": [
    "First let's load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document','another document']\n",
    "\n",
    "training_corpus = []\n",
    "for i, text in enumerate(corpus):\n",
    "    tagged_doc = TaggedDocument(simple_preprocess(text), [i])\n",
    "    ## TODO: Append tagged_doc to the training_corpus you have. \n",
    "    ...\n",
    "    \n",
    "# If you’re running low on RAM, and you know the number of documents ahead of time (your corpus object isn’t an iterator or generator),\n",
    "# you might want to use a preallocated numpy array instead of Python list for your training_corpus:\n",
    "#training_corpus = np.empty(len(corpus), dtype=object);\n",
    "#… \n",
    "#training_corpus[i] = …"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-huntington",
   "metadata": {},
   "source": [
    "Now we will build the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(vector_size=100, min_count=2, workers=num_cores, epochs=10)\n",
    "doc2vec_model.build_vocab(training_corpus)\n",
    "doc2vec_model.train(training_corpus, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7414b4c9-873e-4ea4-a36e-cfa5a3766ceb",
   "metadata": {},
   "source": [
    "### This is how you train a doc2vec model but before you move on to inferencing, \n",
    "### try training the same model with different dimension sizes and by increasing the number of epochs for training. \n",
    "### Observe the result of inferencing via each of these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c645f-6d22-4793-802f-a581f52ae85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Increase the vector size to 300 with this one\n",
    "doc2vec_model_2 = ...\n",
    "doc2vec_model_2.build_vocab(training_corpus)\n",
    "doc2vec_model_2.train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf2519-fa5b-41e5-b6dc-417fd596cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Increase the number of epochs to 20 with this one.\n",
    "doc2vec_model_3 = ...\n",
    "doc2vec_model_3.build_vocab(training_corpus)\n",
    "doc2vec_model_3.train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a91bf1-bf32-47a8-ba26-4bdc11931f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Try increasing both the parameters for this one.\n",
    "doc2vec_model_4 = ...\n",
    "doc2vec_model_4.build_vocab(training_corpus)\n",
    "doc2vec_model_4.train(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-donna",
   "metadata": {},
   "source": [
    "Time to generate the feature vector of a new document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.infer_vector(simple_preprocess('This is a completely unseen document'), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb77a1c-bf2c-46e4-bc9f-10d5876d26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_2.infer_vector(simple_preprocess('This is a completely unseen document'), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa603d-fe9a-4bba-9884-8623af5e6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_3.infer_vector(simple_preprocess('This is a completely unseen document'), epochs=...) # Use the increased number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54668951-fa2a-4949-a8cc-04af29f35861",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_4.infer_vector(simple_preprocess('This is a completely unseen document'), epochs=...) # Use the increased number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-mobility",
   "metadata": {},
   "source": [
    "## Challenge - 5\n",
    "Use the fetch_20newsgroups dataset from sklearn (see code above) and re-train doc2vec with that data instead.\n",
    "\n",
    "Then, check using the most similar function to see if the documents you test are indeed similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-attack",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
